{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: How do you create a PySpark DataFrame from a list or a dictionary?\n",
    "\n",
    "From a List:\n",
    "\n",
    "You can create a PySpark DataFrame from a list of tuples or lists using spark.createDataFrame()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/14 12:14:52 WARN Utils: Your hostname, apples-MacBook-Air-5.local resolves to a loopback address: 127.0.0.1; using 192.168.1.129 instead (on interface en0)\n",
      "25/02/14 12:14:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/14 12:14:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/02/14 12:15:07 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "|  Bob| 45|\n",
      "|Cathy| 29|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"interview_questions\").getOrCreate()\n",
    "\n",
    "# List of tuples\n",
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a Dictionary:\n",
    "\n",
    "You can create a DataFrame from a dictionary by converting it into a list of tuples or using pandas.DataFrame as an intermediate step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "|  Bob| 45|\n",
      "|Cathy| 29|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dictionary\n",
    "data = {\"Name\": [\"Alice\", \"Bob\", \"Cathy\"], \"Age\": [34, 45, 29]}\n",
    "\n",
    "# Using pandas as intermediate\n",
    "import pandas as pd\n",
    "pandas_df = pd.DataFrame(data)\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What are the different ways to filter records in a PySpark DataFrame?\n",
    "\n",
    "    You can filter records in a PySpark DataFrame using:\n",
    "\n",
    "    •\tfilter() or where() methods.\n",
    "\n",
    "    •\tSQL-like expressions or column-based conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "|  Bob| 45|\n",
      "+-----+---+\n",
      "\n",
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "|  Bob| 45|\n",
      "+-----+---+\n",
      "\n",
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "|  Bob| 45|\n",
      "+-----+---+\n",
      "\n",
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using filter() with a condition\n",
    "df.filter(df[\"Age\"] > 30).show()\n",
    "\n",
    "# Using where() with a condition\n",
    "df.where(df[\"Age\"] > 30).show()\n",
    "\n",
    "# Using SQL-like expression\n",
    "df.filter(\"Age > 30\").show()\n",
    "\n",
    "# Multiple conditions\n",
    "df.filter((df[\"Age\"] > 30) & (df[\"Name\"] == \"Alice\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How do you perform pivot and unpivot operations in PySpark?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivot is used to transform rows into columns based on unique values in a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-------+\n",
      "| Name|Math|Science|\n",
      "+-----+----+-------+\n",
      "|  Bob|75.0|   NULL|\n",
      "|Alice|85.0|   90.0|\n",
      "+-----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample DataFrame\n",
    "data = [(\"Alice\", \"Math\", 85), (\"Alice\", \"Science\", 90), (\"Bob\", \"Math\", 75)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Subject\", \"Score\"])\n",
    "\n",
    "# Pivot operation\n",
    "pivot_df = df.groupBy(\"Name\").pivot(\"Subject\").avg(\"Score\")\n",
    "pivot_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unpivot is used to transform columns into rows. \n",
    "\n",
    "PySpark does not have a direct unpivot function, but you can use selectExpr() or stack()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+\n",
      "| Name|Subject|Score|\n",
      "+-----+-------+-----+\n",
      "|  Bob|   Math| 75.0|\n",
      "|  Bob|Science| NULL|\n",
      "|Alice|   Math| 85.0|\n",
      "|Alice|Science| 90.0|\n",
      "+-----+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Unpivot using stack\n",
    "unpivot_df = pivot_df.selectExpr(\n",
    "    \"Name\", \"stack(2, 'Math', Math, 'Science', Science) as (Subject, Score)\"\n",
    ")\n",
    "unpivot_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Explain the difference between withColumn() and select() when modifying columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "withColumn():\n",
    "\n",
    "•\tUsed to add or replace a single column in a DataFrame.\n",
    "\n",
    "•\tReturns a new DataFrame with the added/modified column.\n",
    "\n",
    "•\tSyntax: df.withColumn(\"new_column\", expression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select():\n",
    "\n",
    "•\tUsed to select specific columns or create new columns.\n",
    "\n",
    "•\tCan modify multiple columns at once.\n",
    "\n",
    "•\tSyntax: df.select(\"col1\", \"col2\", expr(\"col3 + 1\").alias(\"new_col\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What are the key differences between DataFrame and Pandas DataFrame in PySpark?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aspect\t        PySpark DataFrame\t                       Pandas DataFrame\n",
    "\n",
    "Execution\tLazy evaluation (optimized execution plan).\tEager evaluation (immediate execution).\n",
    "\n",
    "Scalability\tDistributed and scalable (handles big data).\tSingle-node (limited to memory size).\n",
    "\n",
    "API\tSQL-like, functional programming.\tPythonic, object-oriented.\n",
    "\n",
    "Performance\tOptimized for large datasets.\tOptimized for small to medium datasets.\n",
    "\n",
    "Immutability\tImmutable (operations return new DataFrame).\tMutable (in-place modifications allowed).\n",
    "\n",
    "Ease of Use\tRequires understanding of distributed systems.\tEasier for small-scale data manipulation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you create an RDD from an external file in PySpark?\n",
    "\n",
    "You can create an RDD from an external file (e.g., text file, CSV) using the textFile() or wholeTextFiles() method in PySpark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"RDD Example\")\n",
    "\n",
    "# Create an RDD from a text file\n",
    "rdd = sc.textFile(\"path/to/file.txt\")\n",
    "\n",
    "# Display the first few lines\n",
    "print(rdd.take(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•\ttextFile(): Reads a file and returns an RDD where each element is a line from the file.\n",
    "\n",
    "•\twholeTextFiles(): Reads a file and returns an RDD of key-value pairs, where the key is the file path and the value is the file content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Explain the difference between map() and flatMap() in PySpark RDDs with examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "map():\n",
    "\n",
    "•\tApplies a function to each element of the RDD and returns a new RDD with the transformed elements.\n",
    "\n",
    "•\tThe output RDD has the same number of elements as the input RDD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4])\n",
    "mapped_rdd = rdd.map(lambda x: x * 2)\n",
    "print(mapped_rdd.collect())  # Output: [2, 4, 6, 8]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flatMap():\n",
    "\n",
    "•\tApplies a function to each element of the RDD and returns a new RDD by flattening the results.\n",
    "\n",
    "•\tThe output RDD can have more or fewer elements than the input RDD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([\"Hello World\", \"PySpark is awesome\"])\n",
    "flat_mapped_rdd = rdd.flatMap(lambda x: x.split(\" \"))\n",
    "print(flat_mapped_rdd.collect())  # Output: [\"Hello\", \"World\", \"PySpark\", \"is\", \"awesome\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. What are the advantages and disadvantages of using RDDs over DataFrames?\n",
    "\n",
    "Advantages of RDDs:\n",
    "\n",
    "1.\tFine-Grained Control: RDDs provide low-level APIs for precise control over data transformations and actions.\n",
    "\n",
    "2.\tFlexibility: Supports complex, custom operations that may not be easily expressible in DataFrames.\n",
    "\n",
    "3.\tImmutability: RDDs are immutable, ensuring data consistency in distributed environments.\n",
    "\n",
    "Disadvantages of RDDs:\n",
    "\n",
    "1.\tPerformance: RDDs lack the optimizations (e.g., Catalyst optimizer, Tungsten execution engine) available in DataFrames.\n",
    "\n",
    "2.\tEase of Use: RDDs require more manual effort for common operations compared to DataFrames.\n",
    "\n",
    "3.\tNo Schema: RDDs do not have a schema, making it harder to work with structured data.\n",
    "\n",
    "When to Use RDDs:\n",
    "\n",
    "•\tFor unstructured data (e.g., text, graphs).\n",
    "\n",
    "•\tWhen you need low-level control over transformations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. How do you sort an RDD based on a specific column value?\n",
    "\n",
    "To sort an RDD, use the sortBy() method, which sorts the RDD based on a key function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample RDD\n",
    "rdd = sc.parallelize([(\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29)])\n",
    "\n",
    "# Sort by age (second column)\n",
    "sorted_rdd = rdd.sortBy(lambda x: x[1])\n",
    "print(sorted_rdd.collect())  # Output: [(\"Cathy\", 29), (\"Alice\", 34), (\"Bob\", 45)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Describe how data is distributed across partitions in an RDD and how it impacts performance.\n",
    "\n",
    "Data Distribution in RDDs:\n",
    "\n",
    "•\tAn RDD is divided into partitions, which are distributed across nodes in a cluster.\n",
    "\n",
    "•\tEach partition contains a subset of the data.\n",
    "\n",
    "•\tThe number of partitions is determined by the input data size and the configuration (e.g., spark.default.parallelism).\n",
    "\n",
    "Impact on Performance:\n",
    "\n",
    "1.\tParallelism: More partitions increase parallelism, allowing more tasks to run concurrently.\n",
    "\n",
    "2.\tLoad Balancing: Even distribution of data across partitions ensures balanced workloads.\n",
    "\n",
    "3.\tShuffling: Operations like groupByKey() or reduceByKey() may cause shuffling, which can be expensive in terms of network and disk I/O.\n",
    "\n",
    "4.\tMemory Usage: Too many partitions can lead to overhead, while too few can cause underutilization of resources.\n",
    "\n",
    "Optimizing Partitions:\n",
    "\n",
    "•\tUse repartition() or coalesce() to adjust the number of partitions.\n",
    "\n",
    "•\tAim for partitions of roughly equal size to avoid skew\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. How do you perform a left anti join and left semi join in PySpark SQL?\n",
    "\n",
    "Left Anti Join:\n",
    "\n",
    "•\tReturns only the rows from the left DataFrame that do not have a match in the right DataFrame.\n",
    "\n",
    "•\tSyntax: df1.join(df2, on=\"key\", how=\"left_anti\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "|  2| Bob|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n",
    "df2 = spark.createDataFrame([(1, \"HR\")], [\"id\", \"dept\"])\n",
    "\n",
    "# Left Anti Join\n",
    "result = df1.join(df2, on=\"id\", how=\"left_anti\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left Semi Join:\n",
    "\n",
    "•\tReturns only the rows from the left DataFrame that have a match in the right DataFrame.\n",
    "\n",
    "•\tSyntax: df1.join(df2, on=\"key\", how=\"left_semi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 53:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Left Semi Join\n",
    "result = df1.join(df2, on=\"id\", how=\"left_semi\")\n",
    "result.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
