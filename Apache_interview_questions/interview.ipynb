{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: How do you create a PySpark DataFrame from a list or a dictionary?\n",
    "\n",
    "From a List:\n",
    "\n",
    "You can create a PySpark DataFrame from a list of tuples or lists using spark.createDataFrame()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/14 12:14:52 WARN Utils: Your hostname, apples-MacBook-Air-5.local resolves to a loopback address: 127.0.0.1; using 192.168.1.129 instead (on interface en0)\n",
      "25/02/14 12:14:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/14 12:14:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/02/14 12:15:07 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "|  Bob| 45|\n",
      "|Cathy| 29|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"interview_questions\").getOrCreate()\n",
    "\n",
    "# List of tuples\n",
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a Dictionary:\n",
    "\n",
    "You can create a DataFrame from a dictionary by converting it into a list of tuples or using pandas.DataFrame as an intermediate step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "|  Bob| 45|\n",
      "|Cathy| 29|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dictionary\n",
    "data = {\"Name\": [\"Alice\", \"Bob\", \"Cathy\"], \"Age\": [34, 45, 29]}\n",
    "\n",
    "# Using pandas as intermediate\n",
    "import pandas as pd\n",
    "pandas_df = pd.DataFrame(data)\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What are the different ways to filter records in a PySpark DataFrame?\n",
    "\n",
    "    You can filter records in a PySpark DataFrame using:\n",
    "\n",
    "    •\tfilter() or where() methods.\n",
    "\n",
    "    •\tSQL-like expressions or column-based conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "|  Bob| 45|\n",
      "+-----+---+\n",
      "\n",
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "|  Bob| 45|\n",
      "+-----+---+\n",
      "\n",
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "|  Bob| 45|\n",
      "+-----+---+\n",
      "\n",
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using filter() with a condition\n",
    "df.filter(df[\"Age\"] > 30).show()\n",
    "\n",
    "# Using where() with a condition\n",
    "df.where(df[\"Age\"] > 30).show()\n",
    "\n",
    "# Using SQL-like expression\n",
    "df.filter(\"Age > 30\").show()\n",
    "\n",
    "# Multiple conditions\n",
    "df.filter((df[\"Age\"] > 30) & (df[\"Name\"] == \"Alice\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How do you perform pivot and unpivot operations in PySpark?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivot is used to transform rows into columns based on unique values in a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-------+\n",
      "| Name|Math|Science|\n",
      "+-----+----+-------+\n",
      "|  Bob|75.0|   NULL|\n",
      "|Alice|85.0|   90.0|\n",
      "+-----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample DataFrame\n",
    "data = [(\"Alice\", \"Math\", 85), (\"Alice\", \"Science\", 90), (\"Bob\", \"Math\", 75)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Subject\", \"Score\"])\n",
    "\n",
    "# Pivot operation\n",
    "pivot_df = df.groupBy(\"Name\").pivot(\"Subject\").avg(\"Score\")\n",
    "pivot_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unpivot is used to transform columns into rows. \n",
    "\n",
    "PySpark does not have a direct unpivot function, but you can use selectExpr() or stack()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+\n",
      "| Name|Subject|Score|\n",
      "+-----+-------+-----+\n",
      "|  Bob|   Math| 75.0|\n",
      "|  Bob|Science| NULL|\n",
      "|Alice|   Math| 85.0|\n",
      "|Alice|Science| 90.0|\n",
      "+-----+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Unpivot using stack\n",
    "unpivot_df = pivot_df.selectExpr(\n",
    "    \"Name\", \"stack(2, 'Math', Math, 'Science', Science) as (Subject, Score)\"\n",
    ")\n",
    "unpivot_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Explain the difference between withColumn() and select() when modifying columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "withColumn():\n",
    "\n",
    "•\tUsed to add or replace a single column in a DataFrame.\n",
    "\n",
    "•\tReturns a new DataFrame with the added/modified column.\n",
    "\n",
    "•\tSyntax: df.withColumn(\"new_column\", expression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select():\n",
    "\n",
    "•\tUsed to select specific columns or create new columns.\n",
    "\n",
    "•\tCan modify multiple columns at once.\n",
    "\n",
    "•\tSyntax: df.select(\"col1\", \"col2\", expr(\"col3 + 1\").alias(\"new_col\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What are the key differences between DataFrame and Pandas DataFrame in PySpark?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aspect\t        PySpark DataFrame\t                       Pandas DataFrame\n",
    "\n",
    "Execution\tLazy evaluation (optimized execution plan).\tEager evaluation (immediate execution).\n",
    "\n",
    "Scalability\tDistributed and scalable (handles big data).\tSingle-node (limited to memory size).\n",
    "\n",
    "API\tSQL-like, functional programming.\tPythonic, object-oriented.\n",
    "\n",
    "Performance\tOptimized for large datasets.\tOptimized for small to medium datasets.\n",
    "\n",
    "Immutability\tImmutable (operations return new DataFrame).\tMutable (in-place modifications allowed).\n",
    "\n",
    "Ease of Use\tRequires understanding of distributed systems.\tEasier for small-scale data manipulation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you create an RDD from an external file in PySpark?\n",
    "\n",
    "You can create an RDD from an external file (e.g., text file, CSV) using the textFile() or wholeTextFiles() method in PySpark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"RDD Example\")\n",
    "\n",
    "# Create an RDD from a text file\n",
    "rdd = sc.textFile(\"path/to/file.txt\")\n",
    "\n",
    "# Display the first few lines\n",
    "print(rdd.take(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•\ttextFile(): Reads a file and returns an RDD where each element is a line from the file.\n",
    "\n",
    "•\twholeTextFiles(): Reads a file and returns an RDD of key-value pairs, where the key is the file path and the value is the file content.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
