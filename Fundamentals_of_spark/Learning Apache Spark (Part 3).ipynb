{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ac32257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/10 14:35:43 WARN Utils: Your hostname, apples-MacBook-Air-5.local resolves to a loopback address: 127.0.0.1; using 192.168.1.129 instead (on interface en0)\n",
      "25/02/10 14:35:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/10 14:35:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/02/10 14:35:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/02/10 14:35:45 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/10 14:35:59 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, col, sum, avg, count, first, last, min, max\n",
    "spark = SparkSession.builder.appName(\"sparkApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0d49eb",
   "metadata": {},
   "source": [
    "### Spark Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7c9affe",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data1 = spark.read.csv('Data/log_data1.csv', header=True, inferSchema=True)\n",
    "log_data2 = spark.read.csv('Data/log_data2.csv', header=True, inferSchema=True)\n",
    "log_data3 = spark.read.csv('Data/log_data3.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcbc8645",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------+--------------+-------+\n",
      "|      date|             user_id|   event_name|mobile_os_type|version|\n",
      "+----------+--------------------+-------------+--------------+-------+\n",
      "|12/30/2019|6bcbabd60e6a2e43a...|push_received|        U683CL| 11.3.1|\n",
      "|12/31/2019|313e76b3dbb52b6ce...|push_received|        U683CL| 11.3.1|\n",
      "|12/31/2019|69a8b09b5672fcc5f...|push_received|        U683CL| 11.3.1|\n",
      "|12/31/2019|68413224fbfb9aa33...|push_received|        U683CL| 11.3.1|\n",
      "|12/30/2019|a79f93465f558f433...|push_received|        U683CL| 11.3.1|\n",
      "+----------+--------------------+-------------+--------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_data1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3fb09b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-------------+--------------+-------+\n",
      "|    date|             user_id|   event_name|mobile_os_type|version|\n",
      "+--------+--------------------+-------------+--------------+-------+\n",
      "|1/3/2020|325b61cf1c4dc2a51...|push_received|        U683CL| 11.3.0|\n",
      "|1/4/2020|ad32a2073e48423c2...|  screen_view|        U683CL| 11.3.1|\n",
      "|1/5/2020|05663e0cfd5550ea2...|push_received|        U683CL| 11.3.1|\n",
      "|1/6/2020|8d2f1bc552b468bc1...|push_received|        U683CL| 11.3.1|\n",
      "|1/7/2020|d5474118fa0900557...|push_received|           L51| 11.3.1|\n",
      "+--------+--------------------+-------------+--------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_data3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3afd330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------+------------+\n",
      "|             user_id|         description|       country|items_bought|\n",
      "+--------------------+--------------------+--------------+------------+\n",
      "|6bcbabd60e6a2e43a...|User logged into app|United Kingdom|   HP Laptop|\n",
      "|313e76b3dbb52b6ce...|User logged into app|United Kingdom|   HP Laptop|\n",
      "|69a8b09b5672fcc5f...|User logged into app|United Kingdom|   HP Laptop|\n",
      "|68413224fbfb9aa33...|User logged into app|United Kingdom|   HP Laptop|\n",
      "|a79f93465f558f433...|User logged into app|United Kingdom|   HP Laptop|\n",
      "+--------------------+--------------------+--------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_data2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d9a82a",
   "metadata": {},
   "source": [
    "# Dataframe join syntax\n",
    "df1.join(df2, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72cd9a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+------------+\n",
      "|             user_id|   event_name|         description|items_bought|\n",
      "+--------------------+-------------+--------------------+------------+\n",
      "|6bcbabd60e6a2e43a...|push_received|User logged into app|   HP Laptop|\n",
      "|313e76b3dbb52b6ce...|push_received|User logged into app|   HP Laptop|\n",
      "|69a8b09b5672fcc5f...|push_received|User logged into app|   HP Laptop|\n",
      "|68413224fbfb9aa33...|push_received|User logged into app|   HP Laptop|\n",
      "|a79f93465f558f433...|push_received|User logged into app|   HP Laptop|\n",
      "+--------------------+-------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's perform an inner join\n",
    "log_data1.join(log_data2, log_data1.user_id==log_data2.user_id, 'inner')\\\n",
    ".select(log_data1.user_id, 'event_name', 'description', 'items_bought').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7c35c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-----------+------------+\n",
      "|             user_id|   event_name|description|items_bought|\n",
      "+--------------------+-------------+-----------+------------+\n",
      "|2hhd72fcc3827b456...|   model_view|       NULL|        NULL|\n",
      "|h343aed942faabba1...|push_received|       NULL|        NULL|\n",
      "|456ce1917f02b0051...| push_clicked|       NULL|        NULL|\n",
      "|7823aed942faabba1...|push_received|       NULL|        NULL|\n",
      "|54r1e30403f2ef3c1...|push_received|       NULL|        NULL|\n",
      "|fh5cbd8c37550ba73...|push_received|       NULL|        NULL|\n",
      "|523641efcb0686e0c...|  screen_view|       NULL|        NULL|\n",
      "+--------------------+-------------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's perform a left join where item is null\n",
    "log_data1.join(log_data2, log_data1.user_id==log_data2.user_id, 'left')\\\n",
    ".select(log_data1.user_id, 'event_name', 'description', 'items_bought')\\\n",
    ".where('items_bought IS NULL').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b8e1dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1003"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine dataframe using Union\n",
    "# This allows for duplicate\n",
    "log_with_dup = log_data1.union(log_data3)\n",
    "log_with_dup.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "772c8a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine dataframe using Union\n",
    "# This allows for without duplicate\n",
    "log_without_dup = log_data1.union(log_data3).distinct()\n",
    "log_without_dup.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a739e3a0",
   "metadata": {},
   "source": [
    "### Spark SQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5479d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use the Spark SQL API we need to first create a View/Temporary table from a Dataframe\n",
    "# Let's read Employees data as a view\n",
    "spark.read\\\n",
    ".csv('Data/employees.csv', header=True, inferSchema=True)\\\n",
    ".createOrReplaceTempView('employee_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b938000b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+\n",
      "|     Name| Departments|salary|\n",
      "+---------+------------+------+\n",
      "|    Krish|Data Science| 10000|\n",
      "|    Krish|         IOT|  5000|\n",
      "|   Mahesh|    Big Data|  4000|\n",
      "|    Krish|    Big Data|  4000|\n",
      "|   Mahesh|Data Science|  3000|\n",
      "|Sudhanshu|Data Science| 20000|\n",
      "|Sudhanshu|         IOT| 10000|\n",
      "|Sudhanshu|    Big Data|  5000|\n",
      "|    Sunny|Data Science| 10000|\n",
      "|    Sunny|    Big Data|  2000|\n",
      "+---------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's query our view using Spark SQL\n",
    "# We use the Spark SQL method of the SparkSession\n",
    "employee_view = spark.sql('select * from employee_view')\n",
    "employee_view.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b743d612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+\n",
      "| Departments|highest_salary|\n",
      "+------------+--------------+\n",
      "|         IOT|         15000|\n",
      "|Data Science|         43000|\n",
      "|    Big Data|         15000|\n",
      "+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's perform aggregation and filter like we did before\n",
    "query = '''\n",
    "select Departments, sum(salary) as highest_salary from employee_view\n",
    "group by Departments\n",
    "order by Departments desc;\n",
    "'''\n",
    "dept_salaries_summary = spark.sql(query)\n",
    "dept_salaries_summary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6ad088",
   "metadata": {},
   "source": [
    "### Interoperating the SQL and Dataframe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d20db49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "| Name|TotalSalary|\n",
      "+-----+-----------+\n",
      "|Krish|      19000|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "krish_total_salary = spark.sql('select * from employee_view')\\\n",
    ".where(\"Name = 'Krish'\")\\\n",
    ".groupBy('Name')\\\n",
    ".agg(sum('salary').alias('TotalSalary'))\n",
    "krish_total_salary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf37518",
   "metadata": {},
   "source": [
    "### Writing data to a Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "796175e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Let's write a Dataframe to a parquet file\n",
    "dept_salaries_summary.write.parquet(\"Data/department_salaries_summary.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d2c5d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write a Dataframe to a parquet file with Partition\n",
    "dept_salaries_summary.write.parquet(\"Data/dept_salaries_with_part.parquet\", partitionBy=\"Departments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a02128",
   "metadata": {},
   "source": [
    "### Reading data from Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87209e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+\n",
      "| Departments|highest_salary|\n",
      "+------------+--------------+\n",
      "|         IOT|         15000|\n",
      "|Data Science|         43000|\n",
      "|    Big Data|         15000|\n",
      "+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading data from parquet file into a Dataframe\n",
    "salaries_df = spark.read.parquet(\"data/department_salaries_summary.parquet\")\n",
    "salaries_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91b76a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|highest_salary|\n",
      "+--------------+\n",
      "|         15000|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading data from a specified partition of a parquet file into a Dataframe\n",
    "iot_dept_salaries = spark.read.parquet(\"Data/dept_salaries_with_part.parquet/Departments=Big Data\")\n",
    "iot_dept_salaries.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4a52697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+\n",
      "| Departments|highest_salary|\n",
      "+------------+--------------+\n",
      "|         IOT|         15000|\n",
      "|Data Science|         43000|\n",
      "|    Big Data|         15000|\n",
      "+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM employeeParquetTable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b3ff46",
   "metadata": {},
   "source": [
    "### Convert Spark Dataframe to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e5790afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Departments</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Krish</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Krish</td>\n",
       "      <td>IOT</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mahesh</td>\n",
       "      <td>Big Data</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Krish</td>\n",
       "      <td>Big Data</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mahesh</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>3000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Name   Departments  salary\n",
       "0   Krish  Data Science   10000\n",
       "1   Krish           IOT    5000\n",
       "2  Mahesh      Big Data    4000\n",
       "3   Krish      Big Data    4000\n",
       "4  Mahesh  Data Science    3000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the dataframe above to pandas dataframe\n",
    "employee_df = spark.read.csv('Data/employees.csv', header=True, inferSchema=True)\n",
    "employeePandasDf = employee_df.toPandas()\n",
    "employeePandasDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43620c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
